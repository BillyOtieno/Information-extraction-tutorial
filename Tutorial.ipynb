{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') # download wordnet to be used in lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "py.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll be using the [fake news dataset](https://www.kaggle.com/mrisdal/fake-news) from kaggle for this notebook. The dataset contains text and metadata scraped from 244 websites tagged as \"bullshit\" by the [BS Detector](https://github.com/selfagency/bs-detector) Chrome Extension by [Daniel Sieradski](https://github.com/selfagency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                                text language  \n",
       "0  Print They should pay all the back all the mon...  english  \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  english  \n",
       "2  Red State : \\nFox News Sunday reported this mo...  english  \n",
       "3  Email Kayla Mueller was a prisoner and torture...  english  \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fake = pd.read_csv('fake.csv')\n",
    "df_fake = df_fake[['title', 'text', 'language']].head()\n",
    "df_fake = df_fake.loc[(pd.notnull(df_fake.text)) & (df_fake.language == 'english')]\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process\n",
    "\n",
    "This is one of the most important step in analyzing the text data. If the preprocessing is not good, the algorithm can't do much since we are feeding it a lot of noise, in other words, **Garbage In Garbage Out**. So let's first clean our data using the following techniques:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stopword removal\n",
    "3. Strip punctuation\n",
    "4. Lemmatization\n",
    "5. Bigram collocation detection. (Bigrams are sets of two adjacent tokens. Collocations are frequently co-occurring tokens. Using bigrams, phrases like \"machine_learning\" can be discovered in our output which otherwise would have been treated as two separate words \"machine\" and \"learning\". Spaces are replaced with underscores in corpus)\n",
    "\n",
    "Note: Stemming and lemmatization are two different processes for reducing the morphological variation of words. \n",
    "- Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "\n",
    "- Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "    The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "    The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "    The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/parul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') # download wordnet to be used in lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(texts):\n",
    "    # tokenization\n",
    "    texts = [re.findall(r'\\w+', line.lower()) for line in texts]\n",
    "    # remove stopwords\n",
    "    texts = [remove_stopwords(' '.join(line)).split() for line in texts]\n",
    "    # remove punctuation\n",
    "    texts = [strip_punctuation(' '.join(line)).split() for line in texts]\n",
    "    # remove words that are only 1-2 character\n",
    "    texts = [[token for token in line if len(token) > 1] for line in texts]\n",
    "    # remove numbers\n",
    "    texts = [[token for token in line if not token.isnumeric()] for line in texts]\n",
    "    # lemmatization \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# pre-processing\n",
    "processed_texts = preprocess(df_fake.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['print',\n",
       " 'pay',\n",
       " 'money',\n",
       " 'plus',\n",
       " 'entire',\n",
       " 'family',\n",
       " 'came',\n",
       " 'need',\n",
       " 'deported',\n",
       " 'asap',\n",
       " 'years',\n",
       " 'bust',\n",
       " 'group',\n",
       " 'stealing',\n",
       " 'government',\n",
       " 'taxpayers',\n",
       " 'group',\n",
       " 'somalis',\n",
       " 'stole',\n",
       " 'million',\n",
       " 'government',\n",
       " 'benefits',\n",
       " 'months',\n",
       " 've',\n",
       " 'reported',\n",
       " 'numerous',\n",
       " 'cases',\n",
       " 'like',\n",
       " 'muslim',\n",
       " 'refugees',\n",
       " 'immigrants',\n",
       " 'commit',\n",
       " 'fraud',\n",
       " 'scamming',\n",
       " 'way',\n",
       " 'control',\n",
       " 'related']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# training for bigram collocation detection\n",
    "phrases = Phrases(processed_texts, min_count=1, threshold=0.8, scoring='npmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a performant Phraser object to transform any tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hillary_clinton', 'tree', 'money']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[['hillary', 'clinton', 'tree', 'money']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging detected collocations with data\n",
    "processed_texts = list(bigram[processed_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, holdout and test data\n",
    "training_texts = texts[:5000]\n",
    "holdout_texts = texts[5000:7500]\n",
    "test_texts = texts[7500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary mappings for training data\n",
    "dictionary = Dictionary(training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove rare words and common words based on their document frequency to further prevent noisy results. Below we remove words that appear in less than 10 documents or in more than 60% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute the frequency of each word, including the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function if use all 3 types\n",
    "\n",
    "training_corpus = [dictionary.doc2bow(text) for text in training_texts]\n",
    "holdout_corpus = [dictionary.doc2bow(text) for text in holdout_texts]\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our final corpus looks like in vectorized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document is represented as a list of tuples of (vocab ID, frequency) for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Let's train our topic model which is just a matter of single line with gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training LDA model\n",
    "lda_model = LdaModel(corpus=training_corpus, id2word=dictionary, num_topics=35, passes=50 , chunksize=1500, iterations=200, alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see what the typical output of a topic model looks like. Below, we print top 5 topics. As we can see, each topic is associated with a set of words, and each word has a probability of being expressed under that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other APIs that can be used to explore the trained topic model, for getting further information about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_term_topics` returns the odds of a particular word belonging to some particular topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_term_topics('money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_document_topics` method returns topic distribution of the document along with topic distribution for each word in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic, word_topic, phi_value = model.get_document_topics(training_corpus[0], per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives the topic distribution of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives a ranked list of topics for every word. The lower the rank, more is the probability of the word  belonging to that topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi values are essentially the probability of that word in that document belonging to a particular topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Manual:\n",
    "\n",
    "We would like to know if the correct thing has been learned, does the topics inferred make sense as per our text data.\n",
    "Thanks to pyLDAvis, we can visualise our topic models in a really handy way and inspect what words the topics consist of or how similar the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(model, training_corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what topics does the fake news primarily focuses on, and according to our dataset its mostly about politics. \n",
    "\n",
    "#### Left Panel:\n",
    "\n",
    "1. The area of the circles is proportional to the prevalence of the topics in corpus. So, we can visually determine about the most important topics in our corpus.\n",
    "2. The positioning of the topics is done according to their inter-topic distances, which to some exent preserves the semantic similarity allowing some related topics to form clusters, it is however a little difficult to determine exactly how similar the topics are. For this, we can directly visualize the matrix of inter-topic distances and know the exact distance (with intersecting/different words) between any pair of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_difference(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"\n",
    "    Helper function to plot difference between models\n",
    "    \"\"\"\n",
    "    annotation_html = None\n",
    "    if annotation is not None:\n",
    "        annotation_html = [[\"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                            for (int_tokens, diff_tokens) in row]\n",
    "                           for row in annotation]\n",
    "        \n",
    "    data = Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "    layout = Layout(width=950, height=950, title=title,\n",
    "                       xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "    py.iplot(dict(data=[data], layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix, annotation = model.diff(model, distance='jensen_shannon', num_words=50)\n",
    "plot_difference(difference_matrix, title=\"Topic difference [jensen shannon distance]\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D coordinates which are inferred for each topic in the left panel of pyldavis are based on above distance matrix. [Principal coordinate analysis (PCoA)](https://mb3is.megx.net/gustame/dissimilarity-based-methods/principal-coordinates-analysis) is used for infering the 2d coordinates which seeks to preserve the original topic distances of higher dimensions. You can refer to [this](http://occamstypewriter.org/boboh/2012/01/17/pca_and_pcoa_explained/) blog for getting a graphical reasoning behind this technique.\n",
    "\n",
    "\n",
    "#### Right Panel:\n",
    "\n",
    "1. The terms are initially ranked according to their saliency, when no topic is selected. Saliency basically depends on frequency of the term in corpus, and how informative the specific term w is for determining the individual topics.  For example, if a word w occurs in all topics, observing the word tells us little about the document’s topical mixture <sup>[1]</sup>.\n",
    "2. The tuning parameter, 0≤λ≤1, controls how the terms are ranked for each selected topic, with terms listed in decreasing order of relevance. The relevance of term w to topic t is defined as λ*p(w∣t)+(1−λ)*p(w∣t)/p(w). Values of λ near 1 give high relevance rankings to frequent terms within a given topic, whereas values of λ near zero give high relevance rankings to exclusive terms within a topic i.e. it factors in the general probability of that word in other topics.\n",
    "\n",
    "But how does this ranking relate to the ranking of terms in gensim's `show_topic()` method that we saw above. In gensim, the float value given, next to every term as shown in the list output of `show_topic()` is the value of p(w∣t) i.e. probability of term w for topic t, and gensim simply ranks the topic terms according to this probability value. So, in order to have similar results as gensim’s `show_topic()` in pyLDAvis, λ can be set to 1 which will then result in the relevance being directly proportional to only p(w∣t). \n",
    "\n",
    "Now if we choose λ very close to 1, common terms of the corpus appear near the top for multiple topics, making it hard to differentiate between the meanings of different topics. And if choose λ very close to 0, it can still remain noisy, by giving high rankings to very rare terms that occur in only a single topic, for instance. While such terms may contain useful topic content, but if they are very rare the topic may remain difficult to interpret.\n",
    "\n",
    "The optimal value suggested for λ is 0.6<sup>[2]</sup>.\n",
    "\n",
    "\n",
    "\n",
    "### Automatic:\n",
    "\n",
    "[Coherence](http://qpleple.com/topic-coherence-to-evaluate-topic-models/) is often used to get past the manual inspection and objectively compare the topic models. By returning a score, we can compare between different topic models of the same corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoherenceModel(model, texts=training_texts, dictionary=dictionary, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a really nice explanation for understanding the intution behind coherence that I came across in Matti Lyra's [Pydata Berlin talk](https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb):\n",
    "\n",
    "Take the following two documents that talk about ice hockey. I've highlighted terms that I think are related to the subject matter, you may disagree with my judgement. Notice that among the terms that I've highlighted as being part of the topic of Ice Hockey are words such as Penguin, opposing and shots. None of these on the face of it would appear to \"belong\" to Ice Hockey, but seeing them in context makes it clear that Penguin refers to the ice hockey team, shots refers to disk shaped pieces of vulcanised rubber being launched at the goal at various different speeds and opposing refers to the opposing team although it might more commonly be thought to belong politics or the debate club.\n",
    "\n",
    "> Rinne stopped 27 of 28 **shots** from the **Penguins** in **Game** 6 at home Sunday, but that lone **goal** allowed was enough for the **opposition** to break out the **Stanley Cup trophy** for the second straight **season**.\n",
    "\n",
    "Given the terms that I've determined to be a partial description of Ice Hockey (the concept), one could conceivably measure the coherence of that concept by counting how many times those terms occur with each other i.e. co-occur in some sufficiently large reference corpus.\n",
    "\n",
    "One of course encounters a problem should the reference corpus never refer to ice hockey. A poorly selected reference corpus could for instance be patent applications from the 1800s, it would be unlikely to find those word pairs in that text.\n",
    "\n",
    "This is precisely what several research papers have aimed to do. To take the top words from the topics in a topic model and measure the support for those words forming a coherent concept / topic by looking at the co-occurrences of those terms in a reference corpus. The research up to now was finally wrapped up into a single paper where the authors develop a coherence pipeline, which allows plugging in all the different methods into a single framework. This coherence pipeline is partially implemented in gensim, below is a few examples on how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "In this section, we will go through some of the applications or ways in which we can use topic models and get the most out of them for our NLP tasks.\n",
    "\n",
    "\n",
    "### Document clustering\n",
    "\n",
    "Now apart from getting the topic distribution of a corpus as a whole (as we did using `show_topics`) we can also get the topic distribution of individual documents using `get_document_topics`. Let's see an example for doing it in gensim and then we will visualize the documents based on their topic distribution. That topic distribution representation of each document can be used for clustering the semantically similar documents together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document topics\n",
    "doc_topic, word_topic, phi_value = model.get_document_topics(training_corpus, minimum_probability=0)\n",
    "doc_topic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the topic distribution of first document in the corpus as a list of (topic_id, topic_probability).\n",
    "\n",
    "Now, using the topic distribution of a document as it's vector representation, we will plot all the documents in our corpus using Tensorboard.\n",
    "\n",
    "#### Prepare the Input files for Tensorboard\n",
    "\n",
    "Tensorboard takes two input files, one containing the embedding vectors and the other containing relevant metadata. As described above, we will use the topic distribution of documents as their representative vector and the metadata file will consist of Document titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for tensors(vectors)\n",
    "with open('doc_lda_tensor.tsv','w') as w:\n",
    "    for topics in doc_topics:\n",
    "        w.write(str(topics[1])+ \"\\t\")\n",
    "    w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for metadata(documet titles)\n",
    "with open('doc_lda_metadata.tsv','w') as w:\n",
    "    for doc_id in range(len(doc_topic)):\n",
    "        w.write(\"doc_\" + df_fake.title[doc_id] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go to http://projector.tensorflow.org/ and upload these two files by clicking on Load data in the left panel.\n",
    "\n",
    "Next, we will append the topics with highest probability (topic_id, topic_probability) to the document's title, in order to explore what topics do the cluster corners or edges dominantly belong to. For this, we just need to overwrite the metadata file as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = []\n",
    "for doc_topics in all_topics:\n",
    "    doc_tensor = []\n",
    "    for topic in doc_topics:\n",
    "        if round(topic[1], 3) > 0:\n",
    "            doc_tensor.append((topic[0], float(round(topic[1], 3))))\n",
    "    # sort topics according to highest probabilities\n",
    "    doc_tensor = sorted(doc_tensor, key=lambda x: x[1], reverse=True)\n",
    "    # store vectors to add in metadata file\n",
    "    tensors.append(doc_tensor[:5])\n",
    "\n",
    "# overwrite metadata file\n",
    "with open('doc_lda_metadata.tsv','w') as w:\n",
    "    w.write(\"Doc_Title\\tTopic_dist\\n\")\n",
    "    for doc_id in range(0, len(all_topics)):\n",
    "        w.write(\"doc_%s\\t%s\\n\" % (str(doc_id), str(tensors[doc_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the previous tensor file \"doc_lda_tensor.tsv\" and this new metadata file again to see the updated results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Connections\n",
    "\n",
    "Topic connections can be extremely useful especially for the datasets with interdisciplinary interests. One wide application for topic connections can be seen in semanticscholar.org where they use interrelated topics to index scientific papers.\n",
    "\n",
    "#### Topic Network\n",
    "\n",
    "Networks can be a great way to explore topic models. We can use it to navigate that how topics belonging to one context may relate to some topics in other context and discover common factors between them. We can use them to find communities of similar topics and pinpoint the most influential topic that has large no. of connections or perform any number of other workflows designed for network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topic distributions\n",
    "topic_dist = model.state.get_lambda()\n",
    "\n",
    "# get topic terms\n",
    "num_words = 50\n",
    "topic_terms = [{w for (w, _) in model.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a distance matrix is calculated to store distance between every topic pair. The nodes of the network graph will represent topics and the edges between them will be created based on the distance between two connecting nodes/topics.\n",
    "\n",
    "To draw the edges, we can use different types of distance metrics available in gensim for calculating the distance between every topic pair. Next, we'd have to define a threshold of distance value such that the topic-pairs with distance above that does not get connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gensim.matutils import jensen_shannon\n",
    "import networkx as nx\n",
    "import itertools as itt\n",
    "\n",
    "# calculate distance matrix using the input distance metric\n",
    "def distance(X, dist_metric):\n",
    "    return squareform(pdist(X, lambda u, v: dist_metric(u, v)))\n",
    "\n",
    "topic_distance = distance(topic_dist, jensen_shannon)\n",
    "topic_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store edges b/w every topic pair along with their distance\n",
    "edges = [(i, j, {'weight': topic_distance[i, j]})\n",
    "         for i, j in itt.combinations(range(topic_dist.shape[0]), 2)]\n",
    "\n",
    "# keep edges with distance below the threshold value\n",
    "k = np.percentile(np.array([e[2]['weight'] for e in edges]), 20)\n",
    "edges = [e for e in edges if e[2]['weight'] < k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our edges, let's plot the annotated network graph. On hovering over the nodes, we'll see the topic_id along with it's top words and on hovering over the edges, we'll see the intersecting/different words of the two topics that the edge connects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes and edges to graph layout\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(topic_dist.shape[0]))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "graph_pos = nx.spring_layout(G)\n",
    "\n",
    "# initialize traces for drawing nodes and edges \n",
    "node_trace = Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    text=[],\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=Marker(\n",
    "        showscale=True,\n",
    "        colorscale='YIGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            xanchor='left'\n",
    "        ),\n",
    "        line=dict(width=2)))\n",
    "\n",
    "edge_trace = Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    text=[],\n",
    "    line=Line(width=0.5, color='#888'),\n",
    "    hoverinfo='text',\n",
    "    mode='lines')\n",
    "\n",
    "\n",
    "# no. of terms to display in annotation\n",
    "n_ann_terms = 10\n",
    "\n",
    "# add edge trace with annotations\n",
    "for edge in G.edges():\n",
    "    x0, y0 = graph_pos[edge[0]]\n",
    "    x1, y1 = graph_pos[edge[1]]\n",
    "    \n",
    "    pos_tokens = topic_terms[edge[0]] & topic_terms[edge[1]]\n",
    "    neg_tokens = topic_terms[edge[0]].symmetric_difference(topic_terms[edge[1]])\n",
    "    pos_tokens = list(pos_tokens)[:min(len(pos_tokens), n_ann_terms)]\n",
    "    neg_tokens = list(neg_tokens)[:min(len(neg_tokens), n_ann_terms)]\n",
    "    annotation = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens))), \": \".join((\"---\", str(neg_tokens)))))\n",
    "    \n",
    "    x_trace = list(np.linspace(x0, x1, 10))\n",
    "    y_trace = list(np.linspace(y0, y1, 10))\n",
    "    text_annotation = [annotation] * 10\n",
    "    x_trace.append(None)\n",
    "    y_trace.append(None)\n",
    "    text_annotation.append(None)\n",
    "    \n",
    "    edge_trace['x'] += x_trace\n",
    "    edge_trace['y'] += y_trace\n",
    "    edge_trace['text'] += text_annotation\n",
    "\n",
    "# add node trace with annotations\n",
    "for node in G.nodes():\n",
    "    x, y = graph_pos[node]\n",
    "    node_trace['x'].append(x)\n",
    "    node_trace['y'].append(y)\n",
    "    node_info = ''.join((str(node+1), ': ', str(list(topic_terms[node])[:n_ann_terms])))\n",
    "    node_trace['text'].append(node_info)\n",
    "    \n",
    "# color node according to no. of connections\n",
    "for node, adjacencies in enumerate(nx.generate_adjlist(G)):\n",
    "    node_trace['marker']['color'].append(len(adjacencies))\n",
    "    \n",
    "fig = Figure(data=Data([edge_trace, node_trace]),\n",
    "             layout=Layout(showlegend=False,\n",
    "                hovermode='closest',\n",
    "                xaxis=XAxis(showgrid=True, zeroline=False, showticklabels=True),\n",
    "                yaxis=YAxis(showgrid=True, zeroline=False, showticklabels=True)))\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above graph, we used the 20th percentile of all the distance values as our threshold. But we can experiment with few different values also such that the graph doesn’t become too crowded or too sparse and we could get an optimum amount of information about similar topics or any interesting relations between different topics.\n",
    "\n",
    "### Topic dendrogram\n",
    "\n",
    "The topics can be related to each other in a hierarchical form also. For ex. in case of a research paper corpora, we could have papers belonging to maths, physics, biology which can further be categorised into sub-groups. Maths papers can be sub-divided into topics such as Calculus, Algebra, Geometry; Physics papers into mechanics, electronics, astronomy; Biology into Genetics, Anatomy, Molecular etc.\n",
    "\n",
    "Dendrogram is a tree-structured graph which can be used to visualize the result of a hierarchical clustering. We can use it to explore the topic models and see how the topics are connected to each other in a sequence of successive fusions or divisions that occur in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial as scs\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# get topic distributions\n",
    "topic_dist = model.state.get_lambda()\n",
    "\n",
    "# get topic terms\n",
    "num_words = 300\n",
    "topic_terms = [{w for (w, _) in model.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]\n",
    "\n",
    "# no. of terms to display in annotation\n",
    "n_ann_terms = 10\n",
    "\n",
    "# use Jensen-Shannon distance metric in dendrogram\n",
    "def js_dist(X):\n",
    "    return pdist(X, lambda u, v: jensen_shannon(u, v))\n",
    "\n",
    "# define method for distance calculation in clusters\n",
    "linkagefun=lambda x: sch.linkage(x, 'single')\n",
    "\n",
    "# calculate text annotations\n",
    "def text_annotation(topic_dist, topic_terms, n_ann_terms, linkagefun):\n",
    "    # get dendrogram hierarchy data\n",
    "    linkagefun = lambda x: sch.linkage(x, 'single')\n",
    "    d = js_dist(topic_dist)\n",
    "    Z = linkagefun(d)\n",
    "    P = sch.dendrogram(Z, orientation=\"bottom\", no_plot=True)\n",
    "\n",
    "    # store topic no.(leaves) corresponding to the x-ticks in dendrogram\n",
    "    x_ticks = np.arange(5, len(P['leaves']) * 10 + 5, 10)\n",
    "    x_topic = dict(zip(P['leaves'], x_ticks))\n",
    "\n",
    "    # store {topic no.:topic terms}\n",
    "    topic_vals = dict()\n",
    "    for key, val in x_topic.items():\n",
    "        topic_vals[val] = (topic_terms[key], topic_terms[key])\n",
    "\n",
    "    text_annotations = []\n",
    "    # loop through every trace (scatter plot) in dendrogram\n",
    "    for trace in P['icoord']:\n",
    "        fst_topic = topic_vals[trace[0]]\n",
    "        scnd_topic = topic_vals[trace[2]]\n",
    "        \n",
    "        # annotation for two ends of current trace\n",
    "        pos_tokens_t1 = list(fst_topic[0])[:min(len(fst_topic[0]), n_ann_terms)]\n",
    "        neg_tokens_t1 = list(fst_topic[1])[:min(len(fst_topic[1]), n_ann_terms)]\n",
    "\n",
    "        pos_tokens_t4 = list(scnd_topic[0])[:min(len(scnd_topic[0]), n_ann_terms)]\n",
    "        neg_tokens_t4 = list(scnd_topic[1])[:min(len(scnd_topic[1]), n_ann_terms)]\n",
    "\n",
    "        t1 = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens_t1))), \": \".join((\"---\", str(neg_tokens_t1)))))\n",
    "        t2 = t3 = ()\n",
    "        t4 = \"<br>\".join((\": \".join((\"+++\", str(pos_tokens_t4))), \": \".join((\"---\", str(neg_tokens_t4)))))\n",
    "\n",
    "        # show topic terms in leaves\n",
    "        if trace[0] in x_ticks:\n",
    "            t1 = str(list(topic_vals[trace[0]][0])[:n_ann_terms])\n",
    "        if trace[2] in x_ticks:\n",
    "            t4 = str(list(topic_vals[trace[2]][0])[:n_ann_terms])\n",
    "\n",
    "        text_annotations.append([t1, t2, t3, t4])\n",
    "\n",
    "        # calculate intersecting/diff for upper level\n",
    "        intersecting = fst_topic[0] & scnd_topic[0]\n",
    "        different = fst_topic[0].symmetric_difference(scnd_topic[0])\n",
    "\n",
    "        center = (trace[0] + trace[2]) / 2\n",
    "        topic_vals[center] = (intersecting, different)\n",
    "\n",
    "        # remove trace value after it is annotated\n",
    "        topic_vals.pop(trace[0], None)\n",
    "        topic_vals.pop(trace[2], None)  \n",
    "        \n",
    "    return text_annotations\n",
    "\n",
    "# get text annotations\n",
    "annotation = text_annotation(topic_dist, topic_terms, n_ann_terms, linkagefun)\n",
    "\n",
    "# Plot dendrogram\n",
    "dendro = ff.create_dendrogram(topic_dist, distfun=js_dist, labels=range(1, 36), linkagefun=linkagefun, hovertext=annotation)\n",
    "dendro['layout'].update({'width': 1000, 'height': 600})\n",
    "py.iplot(dendro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The x-axis or the leaves of hierarchy represent the topics of our LDA model, y-axis is a measure of closeness of either individual topics or their cluster. Essentially, the y-axis level at which the branches merge (relative to the \"root\" of the tree) is proportional to their similarity.\n",
    "\n",
    "Text annotations visible on hovering over the merging nodes show the intersecting/different terms of it's two child nodes. Merging node on first hierarchy level uses the topics on leaves directly, to calculate intersecting/different terms, and the upper nodes assume the intersection(+++) as the topic terms of it's child node.\n",
    "\n",
    "This type of tree graph could help us see the high level cluster theme that might exist in our data, as we can see the common/different terms of combined topics in a cluster head annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Coloring\n",
    "\n",
    "We can explore the topic distribution of documents in a visually handy way by coloring its words according to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a sample method to color words.\n",
    "\n",
    "def color_words(model, doc):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # make into bag of words\n",
    "    doc = model.id2word.doc2bow(doc)\n",
    "    # get word_topics\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(doc, per_word_topics=True)\n",
    "\n",
    "    # color-topic matching\n",
    "    topic_colors = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    \n",
    "    # set up fig to plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    # a sort of hack to make sure the words are well spaced out.\n",
    "    word_pos = 1/len(doc)\n",
    "    \n",
    "    # use matplotlib to plot words\n",
    "    for word, topics in word_topics:\n",
    "        ax.text(word_pos, 0.8, model.id2word[word],\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center',\n",
    "                fontsize=20, color=topic_colors[topics[0]],  # choose just the most likely topic\n",
    "                transform=ax.transAxes)\n",
    "        word_pos += 0.2 # to move the word for the next iter\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_words(model, df_fake.text.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Topic Model\n",
    "\n",
    "The author-topic model is an extension of Latent Dirichlet Allocation (LDA), that allows us to learn topic representations of authors in a corpus. The model can be applied to any kinds of labels on documents, such as tags on posts on the web. The model can be used as a novel way of data exploration, as features in machine learning pipelines, for author (or tag) prediction, or to simply leverage your topic model with existing metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data we'll be using consists of scientific papers about machine learning, from the Neural Information Processing Systems conference (NIPS). We crawl the folders and files in the dataset, and read the files into memory.\n",
    "\n",
    "Construct a mapping from author names to document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from smart_open import smart_open\n",
    "\n",
    "# Folder containing all NIPS papers.\n",
    "data_dir = 'nipstxt/'  # Set this path to the data on your machine.\n",
    "\n",
    "# Folders containin individual NIPS papers.\n",
    "yrs = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "dirs = ['nips' + yr for yr in yrs]\n",
    "\n",
    "# Get all document texts and their corresponding IDs.\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for yr_dir in dirs:\n",
    "    files = os.listdir(data_dir + yr_dir)  # List of filenames.\n",
    "    for filen in files:\n",
    "        # Get document ID.\n",
    "        (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start and end of the ID.\n",
    "        doc_ids.append(yr_dir[4:] + '_' + str(int(filen[idx1:idx2])))\n",
    "        \n",
    "        # Read document text\n",
    "        with smart_open(data_dir + yr_dir + '/' + filen, 'rb', encoding='latin-1') as fid:\n",
    "            txt = fid.read()\n",
    "            \n",
    "        # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "        txt = re.sub(r'\\s', ' ', txt)\n",
    "        \n",
    "        docs.append(txt)\n",
    "        \n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "filenames = [data_dir + 'idx/a' + yr + '.txt' for yr in yrs]  # Using the years defined in previous cell.\n",
    "\n",
    "# Get all author names and their corresponding document IDs.\n",
    "author2doc = dict()\n",
    "i = 0\n",
    "for yr in yrs:\n",
    "    # The files \"a00.txt\" and so on contain the author-document mappings.\n",
    "    filename = data_dir + 'idx/a' + yr + '.txt'\n",
    "    for line in smart_open(filename, 'rb', errors='ignore', encoding='latin-1'):\n",
    "        # Each line corresponds to one author.\n",
    "        contents = re.split(',', line)\n",
    "        author_name = (contents[1] + contents[0]).strip()\n",
    "        # Remove any whitespace to reduce redundant author names.\n",
    "        author_name = re.sub(r'\\s', '', author_name)\n",
    "        # Get document IDs for author.\n",
    "        ids = [c.strip() for c in contents[2:]]\n",
    "        if not author2doc.get(author_name):\n",
    "            # This is a new author.\n",
    "            author2doc[author_name] = []\n",
    "            i += 1\n",
    "        \n",
    "        # Add document IDs to author.\n",
    "        author2doc[author_name].extend([yr + '_' + id for id in ids])\n",
    "\n",
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS datast, to an integer ID.\n",
    "doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "# Replace NIPS IDs by integer IDs.\n",
    "for a, a_doc_ids in author2doc.items():\n",
    "    for i, doc_id in enumerate(a_doc_ids):\n",
    "        author2doc[a][i] = doc_id_dict[doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will preprocess this dataset using same process and functions we used earlier for the Fake news dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = preprocess(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = gensim.models.Phrases(docs, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collocation detection\n",
    "processed_docs = [bigram[line] for line in processed_docs]\n",
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(processed_docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The interface to the author-topic model is very similar to that of LDA in Gensim. In addition to a corpus, dictionary (id2word) and number of topics (num_topics), the author-topic model requires either an author to document ID mapping (author2doc), or the reverse (doc2author)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "\n",
    "author_model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring author-topic representation\n",
    "\n",
    "Now that we have trained a model, we can start exploring the authors and the topics.\n",
    "\n",
    "First, let's simply print the top 10 relevant words in the topics. Below we print topic 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_model.show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we give each topic a label based on what each topic seems to be about intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Circuits', 'Neuroscience', 'Numerical optimization', 'Object recognition', \\\n",
    "               'Math/general', 'Robotics', 'Character recognition', \\\n",
    "                'Reinforcement learning', 'Speech recognition', 'Bayesian modelling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just calling `model.show_topics(num_topics=10)`, we format the output a bit so it is easier to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in author_model.show_topics(num_topics=10):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in author_model.show_topic(topic[0]):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are by no means perfect. They have problems such as chained topics, intruded words, random topics, and unbalanced topics ([see Mimno and co-authors 2011](https://people.cs.umass.edu/~wallach/publications/mimno11optimizing.pdf). They will do for the purpose of this tutorial, however.\n",
    "\n",
    "Now let's retrieve the topic distribution for an author. Each topic has a probability of being expressed given the particular author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_model['YannLeCun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the top topics of some authors. First, we make a function to view it more easily and replacing the topic no. by the labels we gave for each topic above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', author_model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in author_model[name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print some high profile researchers and inspect them. Three of these, Yann LeCun, Geoffrey E. Hinton and Christof Koch, are spot on.\n",
    "\n",
    "Terrence J. Sejnowski's results are surprising, however. He is a neuroscientist, so we would expect him to get the \"neuroscience\" label. This may indicate that Sejnowski works with the neuroscience aspects of visual perception, or perhaps that we have labeled the topic incorrectly, or perhaps that this topic simply is not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('YannLeCun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('GeoffreyE.Hinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('TerrenceJ.Sejnowski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_author('ChristofKoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now let's explore our author-topic model using interactive visualizations.\n",
    "\n",
    "We take all the author-topic distributions and embed them in a 2D space. To do this, we reduce the dimensionality of this data using t-SNE.\n",
    "\n",
    "t-SNE is a method that attempts to reduce the dimensionality of a dataset, while maintaining the distances between the points. That means that if two authors are close together in the plot below, then their topic distributions are similar.\n",
    "\n",
    "In the cell below, we transform the author-topic representation into the t-SNE space. You can increase the `smallest_author` value if you do not want to view the authors with few documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [author_model.author2id[a] for a in author_model.author2id.keys() if len(author_model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(author_model.state.gamma[authors, :])  # Result stored in tsne.embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to make the plot.\n",
    "\n",
    "Note that if you run this notebook yourself, you will see a different graph. The random initialization of the model will be different, and the result will thus be different to some degree. You may find an entirely different representation of the data, or it may show the same interpretation slightly differently.\n",
    "\n",
    "If you can't see the plot, you are probably viewing this tutorial in a Jupyter Notebook. View it in an nbviewer instead at http://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [author_model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(author_model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            author_names=author_names,\n",
    "            author_sizes=author_sizes,\n",
    "            radii=radii,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circles in the plot above are individual authors, and their sizes represent the number of documents attributed to the corresponding author. Hovering your mouse over the circles will tell you the name of the authors and their sizes. Large clusters of authors tend to reflect some overlap in interest.\n",
    "\n",
    "We see that the model tends to put duplicate authors close together. For example, Terrence J. Sejnowki and T. J. Sejnowski are the same person, and their vectors end up in the same place (see about (−10,−10) in the plot).\n",
    "\n",
    "At about (−15,−10) we have a cluster of neuroscientists like Christof Koch and James M. Bower.\n",
    "\n",
    "As discussed earlier, the \"object recognition\" topic was assigned to Sejnowski. If we get the topics of the other authors in Sejnoski's neighborhood, like Peter Dayan, we also get this same topic. Furthermore, we see that this cluster is close to the \"neuroscience\" cluster discussed above, which is further indication that this topic is about visual perception in the brain.\n",
    "\n",
    "Other clusters include a reinforcement learning cluster at about (−5,8), and a Bayesian modelling cluster at about (8,−12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. http://vis.stanford.edu/files/2012-Termite-AVI.pdf\n",
    "2. https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
